{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b617333d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#投票法\" data-toc-modified-id=\"投票法-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>投票法</a></span><ul class=\"toc-item\"><li><span><a href=\"#融合模型\" data-toc-modified-id=\"融合模型-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>融合模型</a></span></li><li><span><a href=\"#构建多样性\" data-toc-modified-id=\"构建多样性-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>构建多样性</a></span></li><li><span><a href=\"#分类器加权\" data-toc-modified-id=\"分类器加权-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>分类器加权</a></span></li></ul></li><li><span><a href=\"#堆叠法Stacking\" data-toc-modified-id=\"堆叠法Stacking-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>堆叠法Stacking</a></span><ul class=\"toc-item\"><li><span><a href=\"#细节\" data-toc-modified-id=\"细节-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>细节</a></span></li><li><span><a href=\"#元学习器的特征矩阵\" data-toc-modified-id=\"元学习器的特征矩阵-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>元学习器的特征矩阵</a></span></li><li><span><a href=\"#样本量太少的解决方案：交叉验证\" data-toc-modified-id=\"样本量太少的解决方案：交叉验证-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>样本量太少的解决方案：交叉验证</a></span></li><li><span><a href=\"#特征太少的解决方案\" data-toc-modified-id=\"特征太少的解决方案-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>特征太少的解决方案</a></span></li><li><span><a href=\"#Stacking融合的训练和测试流程\" data-toc-modified-id=\"Stacking融合的训练和测试流程-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Stacking融合的训练和测试流程</a></span><ul class=\"toc-item\"><li><span><a href=\"#stacking的训练:\" data-toc-modified-id=\"stacking的训练:-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span><strong>stacking的训练</strong>:</a></span></li><li><span><a href=\"#stacking的测试\" data-toc-modified-id=\"stacking的测试-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>stacking的测试</a></span></li></ul></li></ul></li><li><span><a href=\"#Blending混合法\" data-toc-modified-id=\"Blending混合法-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Blending混合法</a></span></li><li><span><a href=\"#装袋算法Bagging\" data-toc-modified-id=\"装袋算法Bagging-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>装袋算法Bagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#优缺点\" data-toc-modified-id=\"优缺点-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>优缺点</a></span></li><li><span><a href=\"#OOB策略\" data-toc-modified-id=\"OOB策略-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>OOB策略</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4bc88",
   "metadata": {},
   "source": [
    "**集成学习 (Ensemble Learning)** 通过构建并结合多个学习器来完成学习任务，有时也被称作多分类器系统 (Multi-classifier System)、基于委员会的学习 (Committee-based Learning)等。\n",
    "\n",
    "《机器学习》- 周志华"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e9866",
   "metadata": {},
   "source": [
    "**统计方面**：假设空间不同，泛化能力不同\n",
    "- 集成算法(狭义)：一维假设空间\n",
    "\n",
    ">每个算法都是一套独一无二的、从数据到结论的计算流程，这一流程在统计学上被称为是“对样本与标签关系的假设”。例如，KNN假设，相同类别的样本在特征上应该相似。Boosting算法假设令单一损失函数最小的树一定可以组成令整体损失最小的树林。\n",
    " \n",
    ">我们可以有不同的假设，但在有限的训练数据集上，我们很难看出哪个假设才是对贴近于真实样本标签关系的假设。这就是说，我们无法保证现在表现好的算法在未知数据集上表现也一定会好。因此使用单一假设/单一算法的缺点就很明显：虽然在当前数据集上当前假设表现最好，但如果其他假设才更贴近数据真实的情况，那当前选择出的模型的泛化能力在未来就无法得到保证了。\n",
    " \n",
    "- 模型融合：高维假设空间\n",
    "\n",
    ">相对的，如果能够将不同的假设结合起来，就可以降低选错假设的风险。最贴近数据真实状况的假设，即便在当前数据集上表现并不是最好的，但在融合之后可以给与融合的结果更强大的泛化能力。\n",
    " \n",
    ">同时，假设真正的数据规律并不能被现在的数据代表，那融合多个假设也能够拓展假设空间，不同假设最终能够更接近真实规律的可能性也越大。\n",
    "因此，从统计学的角度来说，模型融合能够：(1) 降低选错假设导致的风险、(2)提升捕捉到真正数据规律的可能性、(3)提升具有更好泛化能力的可能性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a419432",
   "metadata": {},
   "source": [
    "**计算方面**：起点数量不同，模型表现不同\n",
    "- 集成算法(狭义)：只有一组起点，最终只能得到一个局部最小值\n",
    "\n",
    ">许多算法在搜索时都会陷入局部最优解，即便有足够多的训练数据，我们也无法判断能否找到理论上真实的最小值。一般在算法训练时，我们把搜索出的局部最小值当作是真实最小值的近似值，但我们却无法估计真实最小值与局部最小值之间有大多的差异。但由于我们只有一个局部最小值，真实最小值在哪里完全没有头绪。\n",
    " \n",
    "- 模型融合：多组不同的起点，最终得到一个局部最小值的范围\n",
    "\n",
    ">模型融合过程中，每个算法都会有自己的起点、并最终获得多个不同的局部最小值。虽然这些局部最小值当中，没有任意一个等于真正的最小值，但真正的最小值很大可能就在这些局部最小值构成的范围当中。通过结合局部最小值，可以获得更接近真实最小值的近似值，从而提升模型的精度。\n",
    "\n",
    "因此，从计算方面来看，模型融合能够：(1)降低选错起点假设的风险、(2)降低局部最小值远远偏离真实值的风险、(3)提升算法结果更接近真实最小值的可能性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb888342",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221101183239483.png\" alt=\"image-20221101183239483\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d038a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0354e",
   "metadata": {},
   "source": [
    "# 投票法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988c095",
   "metadata": {},
   "source": [
    "适用于分类任务，对多个学习器的预测结果进行投票，即少数服从多数。\n",
    "\n",
    "投票法有两种：**普通投票法和加权投票法**。\n",
    "\n",
    "加权的权重可以人工主观设置或者根据模型评估分数来设置权重。**投票需要3个及3个以上的模型**，同时建议要保证模型的多样性，有时候对同质模型们使用投票法并不能取得较好的表现，这是因为同质模型得到的结果之间可能具有较强的相关性，从而会导致多数人把少数人的好想法给压下去了。\n",
    "\n",
    "为了避免这个问题，可以参考在2014年KDD Cup上Marios Michailid的做法，他对所有结果文件计算Pearson系数，最后选取其中相关性小的模型结果进行投票，分数获得了提升。\n",
    "\n",
    "**投票法对所有评估器输出的结果按类别进行计数，出现次数最多的类别就是融合模型输出的类别**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055eb159",
   "metadata": {},
   "source": [
    "* 绝对多数投票法：至少有50%或以上的分类器都输出了同一类别，否则就拒绝预测, (否则则输出“不确定”，相当于增加一类输出类别)。\n",
    "\n",
    ">绝对多数投票可以帮助我们衡量当前投票的置信程度。票数最多的类别占比越高，说明融合模型对当前样本的预测越有信心。当票数最多的类别占比不足50%时，重新投票对于机器学习来说效率太低，因此我们往往会选择另一种方案：我们可以使用相对较弱的学习器进行绝对多数投票融合、并将绝对多数投票中被输出为“不确定”的样本交由学习能力更强的模型进行预测，相当于将“困难样本”交由复杂度更高的算法进行预测，这也是一种融合方式。\n",
    "\n",
    "* 相对多数投票法：最终结果在投票中票数最多。(**少数服从多数**)\n",
    "\n",
    "* 加权投票法：加权的权重可以人工主观设置或者根据模型评估分数来设置权重。\n",
    "\n",
    "* 硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。\n",
    "\n",
    "* 软投票：增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b13cec",
   "metadata": {},
   "source": [
    ">软投票和硬投票得出的结果可能不同，这是因为软投票有一种“窥探心灵”的能力（也就是衡量置信度的能力）。\n",
    "\n",
    ">在实际进行投票法融合时，我们往往优先考虑软投票 + 相对多数投票方案，因为软投票方案更容易在当前数据集上获得好的分数。然而，如果我们融合的算法都是精心调参过的算法，那软投票方案可能导致过拟合。因此具体在使用时，需要依情况而定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65acc59",
   "metadata": {},
   "source": [
    "## 融合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39207b8",
   "metadata": {},
   "source": [
    "```Voting```可通过```VotingClassifier```和```VotingRegressor```使用。可以将基本模型列表作为参数，列表中的每个模型都必须是具有名称和模型的元组"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22df2d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2022/05/20/qY5UI3gDzSc8p2H.png\" alt=\"image-20220520153651685\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85e9a5",
   "metadata": {},
   "source": [
    "根据少数如从多数的原则，投票得出三个样本的预测结果分别为1、0、1、0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b489c",
   "metadata": {},
   "source": [
    "把这个**少数服从多数的过程等价转化是否超过半数评估器认为该样本应该属于1类**，如果是，则输出结果为1，反之则输出预测结果为0。需要注意的是，该做法会更加方便代码层面的实现，也是后续我们主要采用的计算流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b615239",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2022/05/20/uVrzS79LaBsJgKp.png\" alt=\"image-20220520122231347\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc18896",
   "metadata": {},
   "source": [
    "在单体分类器准确率为80%左右（较为普遍的情况）时，模型投票融合能有平均约15%的准确率提升。当然，该理论实际上是基于分类器相互独立的假设推导而来，而在大多数真实场景下，该假设并不成立，因此该理论的结论可以视作一个理论上限，并不能代表一般情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21000bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb89ea6",
   "metadata": {},
   "source": [
    "评估器核心参数解释如下：\n",
    "|Name|Description|   \n",
    "|:--:|:--:| \n",
    "|estimators|由（评估器名称，评估器）所组成的列表|\n",
    "|voting|融合的方式，包括此前介绍的硬投票和软投票两种{‘hard’,‘soft’}, default=’hard’|\n",
    "|weights|融合过程中各评估器的权重|\n",
    "|flatten_transform|在软投票时打印结果方式|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2384034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#常用工具库\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    " \n",
    "#算法辅助 & 数据\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.datasets import load_digits #分类 - 手写数字数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "#算法（单一学习器）\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNC\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.linear_model import LogisticRegression as LogiR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    " \n",
    "#融合模型\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71531fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_estimators(estimators):\n",
    "    \"\"\"\n",
    "    对模型融合中每个评估器做交叉验证，对单一评估器的表现进行评估\n",
    "    \"\"\"\n",
    "    for estimator in estimators:\n",
    "        cv = KFold(n_splits=5,shuffle=True,random_state=1412)\n",
    "        results = cross_validate(estimator[1],Xtrain,Ytrain\n",
    "                             ,cv = cv\n",
    "                             ,scoring = \"accuracy\"\n",
    "                             ,n_jobs = -1\n",
    "                             ,return_train_score = True\n",
    "                             ,verbose=False)\n",
    "        test = estimator[1].fit(Xtrain,Ytrain).score(Xtest,Ytest)\n",
    "        print(estimator[0]\n",
    "          ,\"\\n train_score:{}\".format(results[\"train_score\"].mean())\n",
    "          ,\"\\n cv_mean:{}\".format(results[\"test_score\"].mean())\n",
    "          ,\"\\n test_score:{}\".format(test)\n",
    "          ,\"\\n\")\n",
    "        \n",
    "def fusion_estimators(clf):\n",
    "    \"\"\"\n",
    "    对融合模型做交叉验证，对融合模型的表现进行评估\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=1412)\n",
    "    results = cross_validate(clf,Xtrain,Ytrain\n",
    "                             ,cv = cv\n",
    "                             ,scoring = \"accuracy\"\n",
    "                             ,n_jobs = -1\n",
    "                             ,return_train_score = True\n",
    "                             ,verbose=False)\n",
    "    test = clf.fit(Xtrain,Ytrain).score(Xtest,Ytest)\n",
    "    print(\"train_score:{}\".format(results[\"train_score\"].mean())\n",
    "          ,\"\\n cv_mean:{}\".format(results[\"test_score\"].mean())\n",
    "          ,\"\\n test_score:{}\".format(test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fe8ac",
   "metadata": {},
   "source": [
    "* train_score：模型在训练集上进行训练的得分表现的均值\n",
    "* cv_mean：模型通过交叉验证在验证集上的得分表现的均值\n",
    "* test_score：训练好的模型在测试集上的得分表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9016bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()     # 使用sklearn自带的手写数字数据集，是一个10分类数据集。\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.2,random_state=1412)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef03b18",
   "metadata": {},
   "source": [
    "**第一组分类器**：放任自由，收敛为主，有较高过拟合风险\n",
    ">逻辑回归+随机森林+梯度提升树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458f8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      " train_score:1.0 \n",
      " cv_mean:0.9666061749903212 \n",
      " test_score:0.9527777777777777 \n",
      "\n",
      "RandomForest \n",
      " train_score:1.0 \n",
      " cv_mean:0.9763574332171892 \n",
      " test_score:0.9833333333333333 \n",
      "\n",
      "GBDT \n",
      " train_score:1.0 \n",
      " cv_mean:0.9686943476577623 \n",
      " test_score:0.9388888888888889 \n",
      "\n",
      "train_score:1.0 \n",
      " cv_mean:0.9777390631049168 \n",
      " test_score:0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "# 构建多组分类器、进行融合\n",
    "clf1 = LogiR(max_iter = 3000,random_state=1412,n_jobs=8)\n",
    "clf2 = RFC(n_estimators= 100,random_state=1412,n_jobs=8)\n",
    "clf3 = GBC(n_estimators= 100,random_state=1412)\n",
    " \n",
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2), (\"GBDT\",clf3)]\n",
    "clf = VotingClassifier(estimators,voting=\"soft\")\n",
    "\n",
    "# 模型评估\n",
    "individual_estimators(estimators)\n",
    "fusion_estimators(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fab31",
   "metadata": {},
   "source": [
    "**通过集成模型的表现有了显著提升，交叉验证分数（97.77%）比任何单一学习器都高，且在测试集上的表现提升到了98.05%，可见当前数据下模型融合的效果十分显著**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc531f08",
   "metadata": {},
   "source": [
    "并不是任意评估器、任意数据上都能够看到如此一目了然的结果。\n",
    "\n",
    "如果你进行投票或平均融合之后，融合的结果反而没有单个算法好，那你可能是落入了投票法与平均法会失效的几大陷阱：\n",
    "\n",
    "---\n",
    "* **评估器之间的学习能力/模型表现差异太大**。在融合中，一个表现很差的模型会拉低整个融合模型的上限，尤其是回归类算法，当一个模型的表现很差时，平均法得出的结果很难比最好的单一算法还好。因此我们必须要使用表现相似的模型进行融合。如果你的评估器中有拖后腿的模型，无论这个模型有多么先进，都应该立刻把它剔除融合模型。\n",
    " ---\n",
    "* **评估器在类型上太相似**，比如、全是树模型、都是Boosting算法，或都是线性评估器等。如果评估器类别太相似，模型融合会发挥不出作用，这在直觉上其实很好理解：如果平均/投票的评估器都一致，那融合模型最终得出的结果也会与单个评估器一致。\n",
    " ---\n",
    "* **对评估器进行了过于精密的调优**。一般来说，我们可能会认为，先对模型进行调优后再融合，能够进一步提升模型的表现。经过粗略调优的评估器融合确实能提升模型表现，但如果对评估器进行过于精密的调优，可能会让融合后的算法处于**严重过拟合**的状态。因此，一般我们不会在评估器上进行太精准的调优。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa50d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840e451",
   "metadata": {},
   "source": [
    "**第二组分类器**：略微调参（非精细化调参），限制过拟合\n",
    "\n",
    "> 模型融合是一个可能加剧过拟合的手段，因此我们必须保证每一个学习器本身的过拟合不严重，为此我们需要对模型进行抗过拟合的处理。需要注意的是，对抗过拟合可能会削弱模型的预测效果，因此我们必须根据过拟合的情况、泛化能力的展现来进行选择。\n",
    "\n",
    "> 对于逻辑回归，我们需要缩小参数C，对随机森林我们选择max_depth，对GBDT我们则选择max_features。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3b2f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      " train_score:1.0 \n",
      " cv_mean:0.9666085946573751 \n",
      " test_score:0.9638888888888889 \n",
      "\n",
      "RandomForest \n",
      " train_score:1.0 \n",
      " cv_mean:0.9763574332171892 \n",
      " test_score:0.9833333333333333 \n",
      "\n",
      "GBDT \n",
      " train_score:1.0 \n",
      " cv_mean:0.9777342237708091 \n",
      " test_score:0.9722222222222222 \n",
      "\n",
      "train_score:1.0 \n",
      " cv_mean:0.9777463221060783 \n",
      " test_score:0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogiR(max_iter = 3000, C=0.1, random_state=1412,n_jobs=8) #这一组合可能说明我们的max_iter设置得太大了\n",
    "clf2 = RFC(n_estimators= 100,max_depth=12,random_state=1412,n_jobs=8)\n",
    "clf3 = GBC(n_estimators= 100,max_features=\"sqrt\",random_state=1412)\n",
    " \n",
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2), (\"GBDT\",clf3)]\n",
    "clf = VotingClassifier(estimators,voting=\"soft\")\n",
    "\n",
    "# 模型评估\n",
    "individual_estimators(estimators)\n",
    "fusion_estimators(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b300f26",
   "metadata": {},
   "source": [
    "根据评估结果:\n",
    "- **GBDT的过拟合调整是非常成功的，交叉验证分数上升的同时，测试集上的分数也得到了大幅提升。**\n",
    "- **Logistic回归**的过拟合调整不明显,交叉验证的分数并未得到明显提升，但测试集上的结果上升，**说明模型的泛化能力变得更加稳定**\n",
    "- **随机森林**的过拟合更加严重，应该取消这种过拟合限制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b4a2d",
   "metadata": {},
   "source": [
    "## 构建多样性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d8a66",
   "metadata": {},
   "source": [
    "在模型融合当中，独立性被称为“多样性”（diversity），评估器之间的差别越大、彼此之间就越独立，因此评估器越多样，独立性就越强。完全独立的评估器在现实中几乎不可能实现，因为不同的算法执行的是相同的预测任务，更何况大多数时候算法们都在相同的数据上训练，因此评估器不可能完全独立。但我们有以下关键的手段，用来让评估器变得更多样、让评估器之间相对独立：\n",
    "\n",
    "- 训练数据多样性：完成多组有效的特征工程，使用不同的特征矩阵训练不同的模型。该方法一般能够得到很好的效果，但如何找出多组有效的特征工程是难题。\n",
    " ---\n",
    "- 样本多样性：使用相同特征矩阵，但每次训练时抽样出不同的样本子集进行训练。当数据量较小时，抽样样本可能导致模型效果急剧下降。\n",
    " ---\n",
    "- 特征样性：使用相同特征矩阵，但每次训练时抽样出不同的特征子集进行训练。当特征量较小时，抽样特征可能导致模型效果急剧下降。\n",
    " ---\n",
    "- 随机多样性/训练多样性：使用相同的算法，但使用不同的随机数种子（会导致使用不同的特征、样本、起点）、或使用不同的损失函数、使用不同的不纯度下降量等。这一方法相当于是在使用Bagging集成。\n",
    " ---\n",
    "- 算法多样性：增加类型不同的算法，如集成、树、概率、线性模型相混合。但需要注意的是，模型的效果不能太糟糕，无论是投票还是平均法，如果模型效果太差，可能大幅度降低融合的结果。\n",
    "---\n",
    ">增加多样性的操作或多或少都阻止了模型学习完整的数据，因此会削弱模型对数据的学习，可能降低模型的效果。因此我们使用多样性时，需要时刻关注着模型的结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bc52e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      " train_score:1.0 \n",
      " cv_mean:0.9666085946573751 \n",
      " test_score:0.9638888888888889 \n",
      "\n",
      "RandomForest \n",
      " train_score:1.0 \n",
      " cv_mean:0.9735699767711964 \n",
      " test_score:0.9777777777777777 \n",
      "\n",
      "GBDT \n",
      " train_score:1.0 \n",
      " cv_mean:0.9777414827719705 \n",
      " test_score:0.9722222222222222 \n",
      "\n",
      "Decision Tree \n",
      " train_score:0.9509369962538313 \n",
      " cv_mean:0.8552821331784747 \n",
      " test_score:0.8527777777777777 \n",
      "\n",
      "KNN \n",
      " train_score:0.9832990502137966 \n",
      " cv_mean:0.9763429152148664 \n",
      " test_score:0.9833333333333333 \n",
      "\n",
      "Bayes \n",
      " train_score:0.8295060354940024 \n",
      " cv_mean:0.8086285327138987 \n",
      " test_score:0.7888888888888889 \n",
      "\n",
      "RandomForest2 \n",
      " train_score:1.0 \n",
      " cv_mean:0.9749491869918699 \n",
      " test_score:0.9833333333333333 \n",
      "\n",
      "GBDT2 \n",
      " train_score:1.0 \n",
      " cv_mean:0.9770446186604724 \n",
      " test_score:0.975 \n",
      "\n",
      "train_score:1.0 \n",
      " cv_mean:0.9826098528842431 \n",
      " test_score:0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "#逻辑回归没有增加多样性的选项\n",
    "clf1 = LogiR(max_iter = 3000, C=0.1, random_state=1412,n_jobs=8)\n",
    "#增加特征多样性与样本多样性\n",
    "clf2 = RFC(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=1412,n_jobs=8)\n",
    "#特征多样性，稍微上调特征数量\n",
    "clf3 = GBC(n_estimators= 100,max_features=16,random_state=1412) \n",
    " \n",
    "#增加算法多样性，新增决策树、KNN、贝叶斯\n",
    "clf4 = DTC(max_depth=8,random_state=1412)\n",
    "clf5 = KNNC(n_neighbors=10,n_jobs=8)\n",
    "clf6 = GaussianNB()\n",
    " \n",
    "#新增随机多样性，相同的算法更换随机数种子\n",
    "clf7 = RFC(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=4869,n_jobs=8)\n",
    "clf8 = GBC(n_estimators= 100,max_features=16,random_state=4869)\n",
    " \n",
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2)\n",
    "              , (\"GBDT\",clf3), (\"Decision Tree\", clf4), (\"KNN\",clf5) \n",
    "              , (\"Bayes\",clf6), (\"RandomForest2\", clf7), (\"GBDT2\", clf8)\n",
    "             ]\n",
    "clf = VotingClassifier(estimators,voting=\"soft\")\n",
    "\n",
    "# 模型评估\n",
    "individual_estimators(estimators)\n",
    "fusion_estimators(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3ddbb",
   "metadata": {},
   "source": [
    "贝叶斯在训练集上的分数很低，这说明模型的学习能力不足\n",
    "\n",
    "因此**剔除表现不良的算法**,并且精简算法多样性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e349ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score:1.0 \n",
      " cv_mean:0.9833067169957413 \n",
      " test_score:0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "estimators = [(\"Logistic Regression\",clf1)\n",
    "              , (\"RandomForest\", clf2)\n",
    "              , (\"GBDT\",clf3)\n",
    "              #, (\"Decision Tree\", clf4)\n",
    "              , (\"KNN\",clf5) \n",
    "              #, (\"Bayes\",clf6)\n",
    "              , (\"RandomForest2\", clf7)\n",
    "              #, (\"GBDT2\", clf8)\n",
    "             ]\n",
    "clf = VotingClassifier(estimators,voting=\"soft\")\n",
    "\n",
    "fusion_estimators(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fdc66",
   "metadata": {},
   "source": [
    "从模型的稳定性来考虑，还是包含随机多样性的组合更好，但为了更快的运算速率，我们可以使用精简多样性继续往下计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9659cf",
   "metadata": {},
   "source": [
    "## 分类器加权"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dddd2",
   "metadata": {},
   "source": [
    "**第一种选项：使用各个模型交叉验证结果本身作为权重，有过拟合风险**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49dcebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score:1.0 \n",
      " cv_mean:0.9826098528842431 \n",
      " test_score:0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2)\n",
    "              , (\"GBDT\",clf3), (\"Decision Tree\", clf4), (\"KNN\",clf5)]\n",
    " \n",
    "clf_weighted = VotingClassifier(estimators,voting=\"soft\",weights=[0.96660,0.97357,0.97774,0.85528,0.97634])\n",
    "\n",
    "fusion_estimators(clf_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21190cb",
   "metadata": {},
   "source": [
    "模型在交叉验证上的效果提升了,并且模型没有陷入过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a611f",
   "metadata": {},
   "source": [
    "**第二种选项：加大效果好的算法的权重，减小效果差的算法的权重。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76af4874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score:1.0 \n",
      " cv_mean:0.9833067169957413 \n",
      " test_score:0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "clf_weighted = VotingClassifier(estimators,voting=\"soft\",weights=[0.95,0.95,0.95,0.85,1.2]) #增大\n",
    "fusion_estimators(clf_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab2dd6",
   "metadata": {},
   "source": [
    "过拟合开始发生了，测试集上的结果开始降低。因此我们选择不加大效果好的算法的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aeaec6",
   "metadata": {},
   "source": [
    "模型达到了目前为止，在5折交叉验证及测试分数上的最高值。\n",
    "\n",
    "现在我们给与决策树的权重非常小，说明现在决策树只是在提供多样性，在实际预测方面做出的贡献较少。提供多样性可以让模型的泛化能力增强，让实际预测方面的贡献变小又可以降低决策树本身较低的预测分数带来的影响。至此，我们就得到了一个很好的融合结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf6920",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03526b",
   "metadata": {},
   "source": [
    "# 堆叠法Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a549f27",
   "metadata": {},
   "source": [
    "作为强学习器的融合方法，Stacking集模型**效果好、可解释性强、适用复杂数据**三大优点于一身，属于融合领域最为实用的先驱方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e7f3d",
   "metadata": {},
   "source": [
    "Stacking结构中有两层算法串联，**第一层叫做level 0，第二层叫做level 1**\n",
    "\n",
    "level 0里面可能包含1个或多个强学习器，而level 1只能包含一个学习器。\n",
    "\n",
    "在训练中，数据会先被输入level 0进行训练，训练完毕后，level 0中的每个算法会输出相应的预测结果。我们将这些预测结果拼凑成新特征矩阵，再输入level 1的算法进行训练。融合模型最终输出的预测结果就是level 1的学习器输出的结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057651e",
   "metadata": {},
   "source": [
    "<center><img src=\"photos\\192b3e5666f04c5d8b563b8484ccdd95.png\" alt=\"image-20220520122231347\" style=\"zoom:83%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da50ab",
   "metadata": {},
   "source": [
    "- **level 0**上训练的多个强学习器被称为**基学习器(base-model)**，也叫做个体学习器。\n",
    "\n",
    "- 在**level 1**上训练的学习器叫**元学习器（meta-model）**。\n",
    "\n",
    ">根据行业惯例，level 0上的学习器是复杂度高、学习能力强的学习器，例如集成算法、支持向量机 (level 0上的算法们的职责是**找出原始数据与标签的关系、即建立原始数据与标签之间的假设，因此需要强大的学习能力**)\n",
    "\n",
    ">而level 1上的学习器是可解释性强、较为简单的学习器，如决策树、线性回归、逻辑回归等 (level 1上的算法的职责是**融合个体学习器做出的假设、并最终输出融合模型的结果**，相当于在寻找“最佳融合规则”，而非直接建立原始数据与标签之间的假设)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab246360",
   "metadata": {},
   "source": [
    "**本质上，level 1的算法只是在学习如何将level 0上输出的结果更好地结合起来，所以Stacking是通过训练学习器来融合学习器结果的方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edddb1",
   "metadata": {},
   "source": [
    "## 细节\n",
    "---\n",
    "- 要不要对融合的算法进行精密的调参？\n",
    "\n",
    ">**个体学习器粗调，元学习器精调**，如果不过拟合的话，可以两类学习器都精调。理论上来说，算法输出结果越接近真实标签越好，但个体学习器精调后再融合，很容易过拟合。\n",
    "\n",
    "- 个体学习器算法要怎样选择才能最大化stacking的效果？\n",
    "\n",
    ">与投票、平均的状况一致，**控制过拟合、增加多样性、注意算法整体的运算时间**。\n",
    "\n",
    "- 个体学习器可以是逻辑回归、决策树这种复杂度较低的算法吗？元学习器可以是xgboost这种复杂度很高的算法吗？\n",
    "\n",
    ">都可以，一切以模型效果为准。对level 0而言，当增加弱学习器来增加模型多样性、且弱学习器的效果比较好时，可以保留这些算法。对level 1而言，只要不过拟合，可以使用任何算法。个人推荐，**在分类的时候可以使用复杂度较高的算法，对回归最好还是使用简单的算法**。\n",
    "\n",
    "- level 0和level 1的算法可不可以使用不同的损失函数？\n",
    "\n",
    ">可以，因为不同的损失函数衡量的其实是类似的差异：即真实值与预测值之间的差异。不过不同的损失对于差异的敏感性不同，如果可能的话建议使用相似的损失函数。\n",
    "\n",
    "- level 0和level 1的算法可不可以使用不同的评估指标？\n",
    "\n",
    ">建议**level 0与level 1上的算法必须使用相同的模型评估指标**。\n",
    "\n",
    ">虽然Stacking中串联了两组算法，但这两组算法的训练却是完全分离的。在深度学习当中，我们也有类似的强大算法串联弱小算法的结构，例如，卷积神经网络就是由强大的卷积层与弱小的线性层串联，卷积层的主要职责是找出特征与标签之间的假设，而线性层的主要职责是整合假设、进行输出。但在深度学习中，一个网络上所有层的训练是同时进行的，每次降低损失函数时需要更新整个网络上的权重。但**在Stacking当中，level 1上的算法在调整权重时，完全不影响level 0的结果**，因此为了保证两组算法最终融合后能够得到我们想要的结果，在训练时一定要以唯一评估指标为基准进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sklearn.ensemble.StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa7fc8",
   "metadata": {},
   "source": [
    "<center><img src=\"photos\\6ce51ea0fcdc471792e90e7c3ef0525a.png\" alt=\"image-20220520122231347\" style=\"zoom:83%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "730b3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#常用工具库\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    " \n",
    "#算法辅助 & 数据\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.datasets import load_digits #分类 - 手写数字数据集\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "#算法（单一学习器）\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNC\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.linear_model import LogisticRegression as LogiR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    " \n",
    "#融合模型\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b44fbb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      " train_score:1.0 \n",
      " cv_mean:0.9666085946573751 \n",
      " test_score:0.9638888888888889 \n",
      "\n",
      "RandomForest \n",
      " train_score:1.0 \n",
      " cv_mean:0.9735699767711964 \n",
      " test_score:0.9777777777777777 \n",
      "\n",
      "GBDT \n",
      " train_score:1.0 \n",
      " cv_mean:0.9777414827719705 \n",
      " test_score:0.9722222222222222 \n",
      "\n",
      "Decision Tree \n",
      " train_score:0.9509369962538313 \n",
      " cv_mean:0.8552821331784747 \n",
      " test_score:0.8527777777777777 \n",
      "\n",
      "KNN \n",
      " train_score:0.9832990502137966 \n",
      " cv_mean:0.9763429152148664 \n",
      " test_score:0.9833333333333333 \n",
      "\n",
      "RandomForest2 \n",
      " train_score:1.0 \n",
      " cv_mean:0.9749491869918699 \n",
      " test_score:0.9833333333333333 \n",
      "\n",
      "GBDT2 \n",
      " train_score:1.0 \n",
      " cv_mean:0.9770446186604724 \n",
      " test_score:0.975 \n",
      "\n",
      "train_score:1.0 \n",
      " cv_mean:0.9805216802168022 \n",
      " test_score:0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "#逻辑回归没有增加多样性的选项\n",
    "clf1 = LogiR(max_iter = 3000, C=0.1, random_state=1412,n_jobs=8)\n",
    "#增加特征多样性与样本多样性\n",
    "clf2 = RFC(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=1412,n_jobs=8)\n",
    "#特征多样性，稍微上调特征数量\n",
    "clf3 = GBC(n_estimators= 100,max_features=16,random_state=1412) \n",
    " \n",
    "#增加算法多样性，新增决策树与KNN\n",
    "clf4 = DTC(max_depth=8,random_state=1412)\n",
    "clf5 = KNNC(n_neighbors=10,n_jobs=8)\n",
    "clf6 = GaussianNB()\n",
    " \n",
    "#新增随机多样性，相同的算法更换随机数种子\n",
    "clf7 = RFC(n_estimators= 100,max_features=\"sqrt\",max_samples=0.9, random_state=4869,n_jobs=8)\n",
    "clf8 = GBC(n_estimators= 100,max_features=16,random_state=4869)\n",
    " \n",
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2)\n",
    "              , (\"GBDT\",clf3), (\"Decision Tree\", clf4), (\"KNN\",clf5) \n",
    "              #, (\"Bayes\",clf6)\n",
    "              , (\"RandomForest2\", clf7), (\"GBDT2\", clf8)\n",
    "             ]\n",
    "\n",
    "#选择单个评估器中分数最高的随机森林作为元学习器\n",
    "#也可以尝试其他更简单的学习器\n",
    "final_estimator = RFC(n_estimators=100\n",
    "                      , min_impurity_decrease=0.0025\n",
    "                      , random_state= 420, n_jobs=8)\n",
    "\n",
    "clf = StackingClassifier(estimators=estimators #level0的7个体学习器\n",
    "                         ,final_estimator=final_estimator #level 1的元学习器\n",
    "                         ,n_jobs=8)\n",
    "\n",
    "# 模型评估\n",
    "individual_estimators(estimators)\n",
    "fusion_estimators(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe587d0c",
   "metadata": {},
   "source": [
    "**数据学习难度较大时**，stacking的优势就会慢慢显现出来。\n",
    "\n",
    "我们现在使用的元学习器几乎是默认参数，我们可以针对元学习器使用贝叶斯优化进行精妙的调参，然后再进行对比，堆叠法的效果可能超越投票法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb659ae1",
   "metadata": {},
   "source": [
    "## 元学习器的特征矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e81bb0",
   "metadata": {},
   "source": [
    "在Stacking过程中，个体学习器会原始数据上训练、预测，再把预测结果排布成新特征矩阵，放入元学习器进行学习。其中，个体学习器的预测结果、即元学习器需要训练的矩阵一般如下排布："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda70f3",
   "metadata": {},
   "source": [
    "<center><img src=\"photos\\93a360e35bd94f8da142b221d3f62695.png\" alt=\"image-20220520122231347\" style=\"zoom:83%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c75067",
   "metadata": {},
   "source": [
    "**新特征矩阵中的特征数就等于个体学习器的个数**。\n",
    "\n",
    ">一般融合模型中个体学习器最多有20-30个，也就是说元学习器的特征矩阵中最多也就20-30个特征。这个特征量对于工业、竞赛中的机器学习算法来说是远远不够的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314e40d",
   "metadata": {},
   "source": [
    "在我们训练stacking模型时，我们一定是将原始数据集分为训练集、验证集和测试集三部分：\n",
    "\n",
    "其中测试集是用于检测整个融合模型的效果的，因此在训练过程中不能使用。\n",
    "\n",
    "而训练集用于训练个体学习器，属于已经完全透露给个体学习器的内容，**如果在训练集上进行预测，那预测结果是“偏高”的、无法代表个体学习器的泛化能力**。\n",
    "\n",
    "因此最后剩下能够用来预测、还能代表个体学习器真实学习水平的，就**只剩下很小的验证集了**。\n",
    ">一般验证集最多只占整个数据集的30%-40%，这意味着元学习器所使用的特征矩阵里的样本量最多也就是原始数据的40%。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb906ffe",
   "metadata": {},
   "source": [
    "## 样本量太少的解决方案：交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a50434",
   "metadata": {},
   "source": [
    "**参数cv，在stacking中执行交叉验证**\n",
    "\n",
    "在stacking方法被提出的原始论文当中，原作者自然也意识到了元学习器的特征矩阵样本量太少这个问题，因此提出了**在stacking流程内部使用交叉验证来扩充元学习器特征矩阵的想法**，即在内部对每个个体学习器做交叉验证，但并不用这个交叉验证的结果来验证泛化能力，而是直接把交叉验证当成了生产数据的工具。\n",
    "\n",
    "\n",
    "具体的来看，在stacking过程中，我们是这样执行交叉验证的，对任意个体学习器来说，假设我们执行5折交叉验证，我们会将训练数据分成5份，并按照4份训练、1份验证的方式总共建立5个模型，训练5次：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7fdc1",
   "metadata": {},
   "source": [
    "<center><img src=\"photos\\997d2ab86f2b44bca2761d5c52a66cf3.png\" alt=\"image-20220520122231347\" style=\"zoom:83%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d04edf",
   "metadata": {},
   "source": [
    "交叉验证的最终输出是5个验证集上的分数，但计算分数之前我们一定是在5个验证集上分别进行预测，并输出了结果。\n",
    "\n",
    "所以我们可以在交叉验证中建立5个模型，轮流得到5个模型输出的预测结果，而这5个预测结果刚好对应全数据集中分割的5个子集。这是说，我们完成交叉验证的同时，也对原始数据中全部的数据完成了预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693618c",
   "metadata": {},
   "source": [
    "**将5个子集的预测结果纵向堆叠，就可以得到一个和原始数据中的样本一一对应的预测结果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cce7ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2)\n",
    "              , (\"GBDT\",clf3), (\"Decision Tree\", clf4), (\"KNN\",clf5) \n",
    "              #, (\"Bayes\",clf6)\n",
    "              , (\"RandomForest2\", clf7), (\"GBDT2\", clf8)\n",
    "             ]\n",
    "final_estimator = RFC(n_estimators=100\n",
    "                      , min_impurity_decrease=0.0025\n",
    "                      , random_state= 420, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56a99d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtest(cv):\n",
    "    clf = StackingClassifier(estimators=estimators\n",
    "                         ,final_estimator=final_estimator\n",
    "                         , cv = cv\n",
    "                         , n_jobs=8)\n",
    "    start = time.time()\n",
    "    clf.fit(Xtrain,Ytrain)\n",
    "    print((time.time() - start)) #消耗时间\n",
    "    print(clf.score(Xtrain,Ytrain)) #训练集上的结果\n",
    "    print(clf.score(Xtest,Ytest)) #测试集上的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2a81933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.755136013031006\n",
      "1.0\n",
      "0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "cvtest(2)     # 较少的验证次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35055c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.262344121932983\n",
      "1.0\n",
      "0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "cvtest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbbc7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.01742649078369\n",
      "1.0\n",
      "0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "cvtest(30)   # 很大的验证次数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2a4ba",
   "metadata": {},
   "source": [
    "## 特征太少的解决方案\n",
    "---\n",
    "参数```stack_method```，更换个体学习器输出的结果类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9eed3a",
   "metadata": {},
   "source": [
    "* 对于分类stacking来说，如果特征量太少，我们可以更换个体学习器输出的结果类型。具体来说，如果个体学习器输出的是具体类别（如[0,1,2]），那1个个体学习器的确只能输出一列预测结果。\n",
    "\n",
    "但如果**把输出的结果类型更换成概率值、置信度等内容**，输出结果的结构一下就可以从一列拓展到多列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e6395",
   "metadata": {},
   "source": [
    "这个行为由参数stack_method控制，这是只有StackingClassifier才拥有的参数，它控制个体分类器具体的输出。\n",
    "\n",
    "```stack_method```里面可以输入四种字符串：```\"auto\", \"predict_proba\", \"decision_function\", \"predict\"```，除了\"auto\"之外其他三个都是sklearn常见的接口。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94092499",
   "metadata": {},
   "source": [
    "> predict_proba：输出概率值;\n",
    "\n",
    "* 对二分类，输出样本的真实标签1的概率，一列\n",
    "* 对n分类，输出样本的真实标签为[0,1,2,3...n]的概率，一共n列\n",
    "\n",
    "> decision_function：每个样本点到分类超平面的距离，可以衡量置信度;对于无法输出概率的算法，如SVM，我们通常它来输出置信度\n",
    "\n",
    "* 对二分类，输出样本的真实标签为1的置信度，一列\n",
    "* 对n分类，输出样本的真实标签为[0,1,2,3...n]的置信度，一共n列\n",
    "\n",
    "> predict：输出具体的预测标签\n",
    "\n",
    "* 对任意分类形式，输出算法在样本上的预测标签，一列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045bbdd3",
   "metadata": {},
   "source": [
    "**对参数stack_method有**：\n",
    "\n",
    ">输入\"auto\"，sklearn会在每个个体学习器上按照\"predict_proba\", \"decision_function\", \"predict\"的顺序，分别尝试学习器可以使用哪个接口进行输出。即，如果一个算法可以使用predict_proba接口，那就不再尝试后面两个接口，如果无法使用predict_proba，就尝试能否使用decision_function。\n",
    "\n",
    "输入三大接口中的任意一个接口名，则默认全部个体学习器都按照这一接口进行输出。然而，如果遇见某个算法无法按照选定的接口进行输出，stacking就会报错。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85566efd",
   "metadata": {},
   "source": [
    "对于回归类算法，我们没有这么多可以选择的接口。回归类算法的输出永远就只有一列连续值\n",
    "\n",
    "**因而我们可以考虑将原始特征矩阵加入个体学习器的预测值，构成新特征矩阵**。\n",
    ">(这个操作有较高的过拟合风险，因此当特征过于少、且stacking算法的效果的确不太好的时候，我们才会考虑这个方案。)\n",
    "\n",
    "控制是否将原始数据加入特征矩阵的参数是```passthrough```。\n",
    "\n",
    ">设置为True时，则将原始特征矩阵加入个体学习器的预测值、构成大特征矩阵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e550be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"Logistic Regression\",clf1), (\"RandomForest\", clf2)\n",
    "              , (\"GBDT\",clf3), (\"Decision Tree\", clf4), (\"KNN\",clf5) \n",
    "              #, (\"Bayes\",clf6)\n",
    "              , (\"RandomForest2\", clf7), (\"GBDT2\", clf8)\n",
    "             ]\n",
    "\n",
    "final_estimator = RFC(n_estimators=100\n",
    "                      , min_impurity_decrease=0.0025\n",
    "                      , random_state= 420, n_jobs=8)\n",
    "\n",
    "clf = StackingClassifier(estimators=estimators\n",
    "                         ,final_estimator=final_estimator\n",
    "                         ,stack_method = \"auto\"\n",
    "                         ,n_jobs=8)\n",
    " \n",
    "clf = clf.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96d506a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 70)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接口transform来查看当前元学习器所使用的训练特征矩阵的结构\n",
    "clf.transform(Xtrain).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54e8f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(estimators=estimators\n",
    "                         ,final_estimator=final_estimator\n",
    "                         ,stack_method = \"auto\"\n",
    "                         ,passthrough = True\n",
    "                         ,n_jobs=8)\n",
    " \n",
    " \n",
    "clf = clf.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc992212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 134)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加入参数passthrough，特征矩阵的特征量会变得更大\n",
    "clf.transform(Xtrain).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6be116c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predict_proba',\n",
       " 'predict_proba',\n",
       " 'predict_proba',\n",
       " 'predict_proba',\n",
       " 'predict_proba',\n",
       " 'predict_proba',\n",
       " 'predict_proba']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 属性stack_method_，我们可以查看现在每个个体学习器都使用了什么接口做为预测输出\n",
    "clf.stack_method_\n",
    "\n",
    "'''\n",
    "7个个体学习器都使用了predict_proba的概率接口进行输出，这与我们选择的算法都是可以输出概率的算法有很大的关系。 \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18fb798",
   "metadata": {},
   "source": [
    "## Stacking融合的训练和测试流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca91910",
   "metadata": {},
   "source": [
    "<center><img src=\"photos\\stacking.png\" alt=\"image-20220520122231347\" style=\"zoom:83%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db698f",
   "metadata": {},
   "source": [
    "### **stacking的训练**:\n",
    "\n",
    "- 将数据分割为训练集、测试集，其中训练集上的样本为𝑀𝑡𝑟𝑎𝑖𝑛，测试集上的样本量为𝑀𝑡𝑒𝑠𝑡\n",
    " ---\n",
    "- 将训练集输入level 0的个体学习器，分别在每个个体学习器上进行交叉验证。在每个个体学习器上，将所有交叉验证的验证结果纵向堆叠形成预测结果。假设预测结果为概率值，当融合模型执行回归或二分类任务时，该预测结果的结构为(𝑀𝑡𝑟𝑎𝑖𝑛,1)，当融合模型执行K分类任务时(K>2)，该预测结果的结构为(𝑀𝑡𝑟𝑎𝑖𝑛,𝐾)\n",
    " ---\n",
    "- **隐藏步骤：使用全部训练数据对所有个体学习器进行训练，为测试做好准备。**\n",
    "---\n",
    "- 将所有个体学习器的预测结果横向拼接，形成新特征矩阵。假设共有N个个体学习器，当融合模型执行回归或二分类任务时，则新特征矩阵的结构为(𝑀𝑡𝑟𝑎𝑖𝑛,𝑁)。如果是输出多分类的概率，那最终得出的新特征矩阵的结构为(𝑀𝑡𝑟𝑎𝑖𝑛,𝑁∗𝐾)\n",
    "---\n",
    "- 将新特征矩阵放入元学习器进行训练。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce4fca",
   "metadata": {},
   "source": [
    "### stacking的测试\n",
    "\n",
    "- 将测试集输入level 0的个体学习器，分别在每个个体学习器上预测出相应结果。假设测试结果为概率值，当融合模型执行回归或二分类任务时，该测试结果的结构为(𝑀𝑡𝑒𝑠𝑡,1)，当融合模型执行K分类任务时(K>2)，该测试结果的结构为(𝑀𝑡𝑒𝑠𝑡,𝐾)\n",
    " ---\n",
    "- 将所有个体学习器的预测结果横向拼接为新特征矩阵。假设共有N个个体学习器，则新特征矩阵的结构为(𝑀𝑡𝑒𝑠𝑡,𝑁)，如果是输出多分类的概率，那最终得出的新特征矩阵的结构为(𝑀𝑡𝑒𝑠𝑡, 𝑁∗𝐾)\n",
    "---\n",
    "- 将新特征矩阵放入元学习器进行预测。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d629e58",
   "metadata": {},
   "source": [
    "因此在stacking中，不仅要对个体学习器完成全部交叉验证，还需要在交叉验证结束后，重新使用训练数据来训练所有的模型。无怪Stacking融合的复杂度较高、并且运行缓慢了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e7ad3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899d298",
   "metadata": {},
   "source": [
    "# Blending混合法\n",
    "---\n",
    "Blending融合是在Stacking融合的基础上改进过后的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1114aa5",
   "metadata": {},
   "source": [
    "**Blending直译为“混合”，但它的核心思路其实与Stacking完全一致：**\n",
    "\n",
    ">使用两层算法串联，level 0上存在多个强学习器，level 1上有且只有一个元学习器，且level 0上的强学习器负责拟合数据与真实标签之间的关系、并输出预测结果、组成新的特征矩阵，然后让level 1上的元学习器在新的特征矩阵上学习并预测。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fb17a",
   "metadata": {},
   "source": [
    "# 装袋算法Bagging\n",
    "---\n",
    "```Bagging```通过采样训练数据集的样本，训练得到多样的模型，进而得到多样的预测结果。\n",
    "\n",
    "在结合模型的预测结果时，可以对单个模型预测结果进行投票或平均。\n",
    ">Bagging的**关键**是对数据集的采样方法.常见的方式可以从行（样本）维度进行采样，这里进行的是有放回采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c20a1b",
   "metadata": {},
   "source": [
    "```Bagging```可通过```BaggingClassifier```和```BaggingRegressor```使用，默认情况下它们使用决策树作为基本模型，可以通过n_estimators参数指定要创建的树的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a9636",
   "metadata": {},
   "source": [
    "|参数\t|说明|\n",
    "|----|----|\n",
    "|base_estimator\t|object, default=None 基本估计量适合数据集的随机子集。如果为None，则基本估计量为决策树。|\n",
    "|n_estimators\t|int，default = 10 集合中基本估计量的数量。|\n",
    "|max_samples\t|int or float, default=1.0 从X抽取以训练每个基本估计量的样本数量.如果为int，则抽取max_samples样本。;如果为float，则抽取样品。max_samples * X.shape[0]。|\n",
    "|max_features\t|int or float, default=1.0从X绘制以训练每个基本估计量的要素数量;如果为int，则绘制max_features特征。;如果为float，则绘制特征。max_features * X.shape[1]|\n",
    "|bootstrap\t|bool, default=True是否抽取样本进行替换。如果为False，则执行不替换的采样。|\n",
    "|bootstrap_features\t|bool, default=False是否用替换绘制特征。|\n",
    "|oob_score|bool，defalut = False是否使用现成的样本来估计泛化误差。|\n",
    "|warm_start|\tbool，defalut = False设置为True时，请重用上一个调用的解决方案以适合并在集合中添加更多估计量，否则，仅适合一个全新的集合。请参阅Glossary。|\n",
    "|n_jobs\t|int, default=None (fit和 并行运行的作业数predict)|\n",
    "|verbose\t|int, default=0在拟合和预测时控制冗余程度。|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e77132",
   "metadata": {},
   "source": [
    "**原理过程**:\n",
    "\n",
    "- 将原始数据集D随机划分成m个子集D1, D2, …, Dm\n",
    "- 对于每个子集Di, 训练一个基学习器Hi\n",
    "- 对于每个测试样本x，将其输入到所有的基学习器Hi中，并得到对应的预测结果{yi1, yi2, …, yim}\n",
    "- 将所有的预测结果进行结合，以得出最终的预测结果y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3b527",
   "metadata": {},
   "source": [
    "<center><img src=\"photos\\927391-20160717135005498-1140287801.png\" alt=\"image-20220520122231347\" style=\"zoom:83%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc2da4",
   "metadata": {},
   "source": [
    "## 优缺点\n",
    "Bagging算法**优点**：\n",
    "\n",
    "- Bagging算法可以显著降低模型的方差，提高模型的稳定性。通过使用多个相互独立的学习器，Bagging可以减少模型对训练数据的敏感性，从而更好地适应未知的测试数据。\n",
    "- Bagging算法可以并行计算，加速模型训练的速度。\n",
    "- Bagging算法不容易出现过拟合。通过使用随机抽样的方式来生成多个子集\n",
    "\n",
    "Bagging算法的**缺点**包括：\n",
    "\n",
    "- Bagging算法对于噪声数据比较敏感。由于Bagging算法对于每个子集都会训练一个基学习器，因此如果某个子集**包含大量的噪声数据**，那么对应的基学习器的性能可能会下降，从而影响整个模型的性能。\n",
    "- Bagging算法在处理分类问题时，容易出现**过于一致性问题**。如果不同的基学习器都对同一个测试样本做出了**相同的错误预测**，那么Bagging算法将无法有效纠正这个错误。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd259cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris=load_iris()\n",
    "\n",
    "X=iris.data[:,2:4]\n",
    "y=iris.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81d7c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: 0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36085\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bgg_clf=BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=10,\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bgg_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=bgg_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('准确率:',accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63eb52",
   "metadata": {},
   "source": [
    "## OOB策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e901447",
   "metadata": {},
   "source": [
    "**OOB（Out-Of-Bag）策略是Bagging算法的一种特殊形式，可以用于评估Bagging模型的性能和特征重要性**。\n",
    "\n",
    "使用oob_score_属性来获取模型的OOB评估结果。使用OOB策略可以提高Bagging算法的性能，并在一定程度上避免了过拟合的问题。\n",
    "\n",
    "同时，通过统计每个特征在OOB样本上的预测结果，我们还可以评估特征的重要性，从而进一步优化模型的性能。\n",
    "\n",
    "需要注意的是，在使用OOB策略时，我们需要确保每个基学习器都使用了不同的子样本进行训练。如果使用相同的子样本进行训练，那么在OOB样本中可能会出现重复样本，导致评估结果不准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476591d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36085\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgg_clf=BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=10,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "bgg_clf.fit(X_train,y_train)\n",
    "bgg_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a760ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=bgg_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e705d9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.04487179, 0.95512821],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.94425445, 0.05574555],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01282051, 0.98717949],\n",
       "       [0.        , 0.02106722, 0.97893278],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01081081, 0.98918919],\n",
       "       [0.        , 0.96376812, 0.03623188],\n",
       "       [0.        , 0.01282051, 0.98717949],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.96956111, 0.03043889],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.93435635, 0.06564365],\n",
       "       [0.        , 0.47471169, 0.52528831],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.0231982 , 0.9768018 ],\n",
       "       [0.        , 0.01388889, 0.98611111],\n",
       "       [0.        , 0.02358368, 0.97641632],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01907051, 0.98092949],\n",
       "       [0.        , 0.79130024, 0.20869976],\n",
       "       [0.        , 0.95564262, 0.04435738],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.0231982 , 0.9768018 ],\n",
       "       [0.        , 0.0228022 , 0.9771978 ],\n",
       "       [0.        , 0.94666032, 0.05333968],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.89130435, 0.10869565],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.89130435, 0.10869565],\n",
       "       [0.        , 0.78625323, 0.21374677],\n",
       "       [0.        , 0.14622315, 0.85377685],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.00625   , 0.99375   ],\n",
       "       [0.        , 0.62950296, 0.37049704],\n",
       "       [0.        , 0.96631327, 0.03368673],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.48837209, 0.51162791],\n",
       "       [0.        , 0.96631327, 0.03368673],\n",
       "       [0.        , 0.95588235, 0.04411765],\n",
       "       [0.        , 0.96657167, 0.03342833],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.02635135, 0.97364865],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9628363 , 0.0371637 ],\n",
       "       [0.        , 0.01025641, 0.98974359],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01081081, 0.98918919],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.02083333, 0.97916667],\n",
       "       [0.        , 0.90153453, 0.09846547],\n",
       "       [0.        , 0.97212722, 0.02787278],\n",
       "       [0.        , 0.65116279, 0.34883721],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.97416835, 0.02583165],\n",
       "       [0.        , 0.02708333, 0.97291667],\n",
       "       [0.        , 0.96318083, 0.03681917],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.02106722, 0.97893278],\n",
       "       [0.        , 0.96376812, 0.03623188],\n",
       "       [0.        , 0.01976351, 0.98023649],\n",
       "       [0.        , 0.01525641, 0.98474359],\n",
       "       [0.        , 0.97238562, 0.02761438],\n",
       "       [0.        , 0.03773389, 0.96226611],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.95601618, 0.04398382],\n",
       "       [0.        , 0.02083333, 0.97916667],\n",
       "       [0.        , 0.03247748, 0.96752252],\n",
       "       [0.        , 0.97212722, 0.02787278],\n",
       "       [0.        , 0.92239636, 0.07760364],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.02747748, 0.97252252],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.9687953 , 0.0312047 ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.93454106, 0.06545894],\n",
       "       [0.        , 0.99444444, 0.00555556],\n",
       "       [0.        , 0.96895854, 0.03104146],\n",
       "       [0.        , 0.74468085, 0.25531915],\n",
       "       [0.        , 0.97674419, 0.02325581],\n",
       "       [0.        , 0.04716736, 0.95283264],\n",
       "       [0.        , 0.01351351, 0.98648649],\n",
       "       [0.        , 0.03561157, 0.96438843],\n",
       "       [0.        , 0.99468085, 0.00531915],\n",
       "       [0.        , 0.99109352, 0.00890648],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.96610589, 0.03389411],\n",
       "       [0.        , 0.98863049, 0.01136951],\n",
       "       [0.        , 0.97740393, 0.02259607],\n",
       "       [0.        , 0.98281654, 0.01718346],\n",
       "       [0.        , 0.01907051, 0.98092949],\n",
       "       [0.        , 0.97416835, 0.02583165],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01907051, 0.98092949],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9891253 , 0.0108747 ],\n",
       "       [0.        , 0.24418605, 0.75581395]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgg_clf.oob_decision_function_     #表示每个类别所属的概率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "331.865px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
